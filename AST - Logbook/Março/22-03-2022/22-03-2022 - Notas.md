# Machine Learning - Google Crash Course
## Linear Regression with Synthetic Data - Exercise
This exercise studies the influence of hyperparameters such as (learning rate, number of epochs and batch size) in the overall performance of a model.

An **epoch** is a full training pass over the **entire data set,** such that each example (item in the data set) has been seen once. Therefore, an epoch represents N/batch size training iterations (one iteration means that the weights have been updated one time), where N is the total number of examples.

In the case of the exercise, we have a data set with **12 examples**, this means that:
- For a batch size of **1**, there will be $\frac{12}{1}$ iterations and the **weights will be updated 12 times**.
- On the other end, for a batch size of **12**, there will be 1 iteration and the **weights will be updated just one time**.

To support some conclusions achieved in [[20-03-2022 - Notas#^7d6b18 | 20-03-2022]], here are a few rules of thumb:
- Training loss should steadily decrease, steeply at first and then more gradually (until it approaches zero);
- If the training loss **doesn't converge**, train for more epochs;
- If the training loss is **decreasing too slowly**, the learning rate needs to be increased. *BUT* if it is set too high, the loss function might oscillate and never converge.
- If the training loss is **varies too much** (oscillator behavior), then the learning rate needs to be decreased.
- **Lowering the learning rate** while **increasing the number of epochs or batch size** is often a good combination for a more efficient training.
- A **batch size too low** can also cause instability. First, we should try with bigger values and gradually decrease it until degradation appears.
- For real-world data sets with a **lot of examples**, it might not fit into memory. Therefore, we need to reduce the batch size, so it can fit into memory.

-> The **ideal combination** of hyperparameters is data dependent, consequently, experimentation is mandatory.

---
Tags:
[[21-03-2022]], [[Notas]], [[Curso]]