# Machine Learning - Google Crash Course
## Validation Set
In the previous topic, we discussed an approach to train a model in a situation where we only have one data set available. The data set would be divided into two, where one would be called the training set and the other the test set. The model would train on the training set and **evaluate** on the test set, then taking these results to tweak the model's hyperparameters (learning rate and features). 

The **caveat** on this iterative approach of training and evaluating is, **after many rounds, the procedure might cause us to implicitly fit to the peculiarities of the test set**. The more often a given test set is evaluated, the higher the risk of implicitly overfitting to that one test set is. **Let's look at a better protocol**.

## Another Partition
To recollect about the previously mentioned method, let's look at the following workflow scheme:

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/WorkflowWithTestSet.svg"/>
</p>

The data set would be divided into a set for training and a set to make evaluations. In the "Tweak model according to results on Test Set", it means changing what it's possible on the model in order for it to give better results on the test set. It could be removing features, tweaking the batch size, changing the learning rate or even designing a new model from scratch. In the end, the model should be the best possible for the test set.

Dividing the data set into two partitions was a great idea, but it is not a panacea ("one size fit's all" type of thing), like we've seen section before where we discussed its caveat.

Now, let's delve in another proposition to dividing the data set:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/PartitionThreeSets.svg"/>
</p>

Dividing the set into three subsets by including a **validation set**, the overfitting problem is greatly reduced, as we can properly evaluate the model. The following figure represents how the workflow processes:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/WorkflowWithValidationSet.svg"/>
</p>

This improved method won't evaluate on the test set, therefore, there's no chance of molding the model to the peculiarities of said set and "rig" the results. The validation set allows us to adjust the hyperparameters of the model, and the test set will act as a double-check instance **after** the model passes the validation.

The improved workflow:
1. Picks the model that does best on the validation set.
2. Double-checks that model against set.

This workflow is better, since the model is less exposed to the test set, allowing apt generalization since there's a lesser risk of occuring overfitting.



---
Tags:
[[21-03-2022]], [[Notas]], [[Curso]]