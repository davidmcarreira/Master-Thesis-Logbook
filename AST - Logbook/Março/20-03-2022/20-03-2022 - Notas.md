# Machine Learning - Google Crash Course
## Training and Test Sets
As mentioned before, a test set is a data set that is used to evaluate the performance of a model trained by a training set.

If we only have one data set available, then we need to partition it as follows:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/PartitionTwoSets.svg">
</p>

Before separating, usually it is a good measure to **randomize** the whole set in the first place in order to guarantee that specific types of data don't end up in only one of the divisions (all the cats end up in the training set and the test set is composed of all the other animals in the data set).

The **test set** needs to meet the 2 following requirements:
1. It needs to be large enough to yield statistically meaningful results.
2. It is representative of the data set as a whole, i.e, as the same characteristics as the training set.

<br>

Once again, we have the generalization question is raised. We need to assure that the model generalizes well to new data, therefore, the test set will act as a great proxy for that said data. Let's see an example of a model that does good in the training data as well in the test data:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/TrainingDataVsTestData.svg" style="background-color: red;">
</p>

Albeit it is a very simple model, it does a fairly good job in predicting the outcomes in both sets. This fact gives us some degree of confidence to deploy it in new random data.
&nbsp;
 
A very important thing to have in mind is to **never train the model on test data**, it will completely manipulate the results, leading us to believe that we have an 100% success rate and 0 loss. And also, every time the results are looking very promising (too good to be true, even), they shall be approached with care as they might indicate that test data has leaked into the training set.
 
After training, the model achieves 99% precision on both the training set and the test set. We'd expect a lower precision on the test set, so we take another look at the data and discover that many of the examples in the test set are duplicates of examples in the training set (we neglected to scrub duplicate entries for the same spam email from our input database before splitting the data). We've inadvertently trained on some of our test data, and as a result, we're no longer accurately measuring how well our model generalizes to new data.
 
&nbsp;
&nbsp;

<u>Example</u>:
 For example, let's consider a model that predicts if an email is considered spam, through features as the subject line, email body and sender's email. We partition the data set in an 80-20 split for training and test data, respectively.  After training, we see a 99% precision on both the training and test set, which look too optimistic. After taking a look at the data, we discover that there are duplicates in the test set that were also in the training data. Basically, we committed the biggest sin of them all: trained in the test set. Inadvertently, yes, but now the model can't be trusted and needs to be trained **all over again**.
 
 
 
---  
Tags:
[[20-03-2022]], [[Notas]], [[Curso]]