# Machine learning Crash Course - Google 
## Reducing Loss
In order to train a model, loss needs to be reduced. An iterative approach is one widely applied method, being it is easy and efficient.

### An Iterative Approach
Iterative learning is the process of searching for the lowest loss possible by changing the values of the weights, and then apply the model again. It's a trial-and-error process, as suggested by the next image:

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentDiagram.svg">
</p>

Iterative strategies are prevalent in machine learning since their great scalability with large data sets. For a model such as:
$$
y'=b+w_1x_1
$$
 where $y'$ is the predicted output, $b$ is the bias, $w_1$ is the weight and $x_1$ the feature (input). To start minimizing the loss, first random values are attributed to the weight and bias:
 - $b=0$
 - $w_1=0$

If the first value of the feature is 10, then the predicted outcome will be 0. Taking into consideration, the loss function value is computed and new values for $b$ and $w_1$ are generated ("Computer parameter updates" stage in the previous figure).

The learning process continues until the algorithm finds the best model parameters possible that gives the lowest possible loss. Usually, the iteration occurs until the loss stops changing or changes extremely slowly. At that point, it's said that the model **converged**.

### Gradient Descent
<p>
	<img src="https://developers.google.com/machine-learning/crash-course/images/convex.svg">
</p>

Convex problems have only one minimum (has seen in the previous figure), but that usually is not the case.

Calculating the loss function for every value of $w_1$ over the entire data set is computationally inconceivable, as sometimes they're constituted by millions (or billions) of entries, rendering the process useless. A better mechanism (very popular in machine learning) is the **gradient descent**.

The first stage in **gradient descent** is to pick a starting point of $w_1$ and work from there (for many algorithms the value itself doesn't really matter, so usually it is set to the trivial value 0 or a random value).

<p>
	<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentStartingPoint.svg">
</p>

The gradient descent algorithm is applied and calculates the gradient of the loss curve, i.e, in what direction does the function vary the most. In this case the last statement is not very useful, but in multidimensional functions it is a very important notion. The **gradient** is a vector of partial derivatives with **respect to the weight**, so it has a **direction** and a **magnitude**.

The gradient will point the direction of the steepest increase (or decrease) in the loss function. Considering a positive gradient will dictate the greatest increase, while a negative gradient will give the greatest decrease, giving name to the algorithm - gradient **descent**.

<p>
	<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentNegativeGradient.svg">
</p>

The next point is calculated by multiplying the previous point's gradient vector by a determined factor (Learning Rate):

<p>
	<img src="https://developers.google.com/machine-learning/crash-course/images/GradientDescentGradientStep.svg">
</p>

This process is repeated, edging closer to the minimum (where the gradient vector is zero, since the derivative of an extreme is zero).



---
Tags:
[[17-03-2022]], [[Notas]], [[Curso]]