#  Report
- Regularization:
	- [[11-05-2022 - Notas#Regularization for simplicity L_2 Regularization|L_1]]
	- [[19-05-2022 - Notas#Regularization for Sparsity L_1 Regularization|L_2]]
	- [[19-05-2022 - Notas#Dropout regularization|Dropout]].
- [[12-05-2022 - Notas#Logistic Regression Calculating a Probability|Logistics Regression]]
- [[14-05-2022 - Notas#Classification Thresholding|Classification]]
	- [[14-05-2022 - Notas#Classification Accuracy| Classification metrics]]
- [[19-05-2022 - Notas#Neural Networks Structure|Neural Network's structure]]

[Neural Networks, Manifolds and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

[Backpropagation](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll)

&nbsp;

&nbsp;


---
# Machine Learning - Google Crash Course
## Training Neural Networks: Best Practices
### Failure Cases
There are numerous ways for **backpropagation** to go wrong.

### Vanishing Gradients
The gradients for the layers closer to the input can become very small, and computing them might involve taking the product of many small terms. When the gradients vanish toward zero, these lower layers end up training very slowly (or not training at all). One way of preventing that is using **ReLU** as the **activation function**.

### Exploding Gradients
Gradients vanishing is a problem, but things can go in the opposite direction. If the weights are very large, then the gradients for the lower layers will involve the product of large terms and the gradients will never converge. Performing **batch normalization** or **lowering the learning rate** are good ways of preventing exploding gradients.

### Dead ReLU Units
When the weighted sum for a ReLU falls below zero, the activation function will get stuck by outputting zero activation, and not contributing to the network's output. Since the backpropagation can't proceed, the gradients will not flow during training, resulting in the probability of the input of the ReLU not changing enough to bring the weighted sum back above zero. **Lowering the learning rate** can help keep the ReLU from dying.

### Dropout Regularization
Another form of regularization is called **Dropout Regularization**, and is useful nor Neural Networks only. The way it works is by randomly *dropping* a unit of activation in a network during a single gradient step. The more it is dropped, the stronger the regularization:
- 0.0 -> No dropout reg;
- 1.0 -> Dropout everything (the model learns nothing);
- 0.5 -> Drop only half of the units.

The dropout values should stay between 0 and 1 to work.

---
Tags:
[[20-05-2022]], [[Curso]], [[Reuni√£o]]