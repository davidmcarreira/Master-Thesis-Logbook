
# Machine Learning - Google Crash Course
## Regularization for Sparsity: $L_1$ Regularization
Feature crosses are very useful, however, they result in more dimensions for our problem. With that, the model can grow too big and consequently so does memory usage cost, and that is a problem.

A good solution would be to drive to **zero** uninformative weights every time it's possible, since it removes completely the corresponding feature, saving RAM and potentially reducing noise in the model.

Previously, we studied $L_2$ regularization, but one of its characteristics is that it **never** drives the weight to exactly zero, so not an option.

There's also $L_0$ regularization, but it's also not an option, as it penalizes the count of non-zero coefficients values and increases the count if there was a sufficient gain in the model's ability to fit the data, that turns a convex optimization to a non-convex optimization problem.

Finally, there's another term, the $L_1$ regularization, that serves the same purpose as $L_0$, but has the advantage of being convex and thus efficient to compute. $L_1$ will drive the less informative weights to zero, reaping RAM savings.

### $L_1$ vs $L_2$ regularization

The way both terms penalize weights are different:
- $L_2$ penalizes weight $^2$;
- $L_1$ penalizes $\vert$weight$\vert$.

Consequently, they have different derivatives with different meanings:
- Deriving $L_2$ we get $2\times\text{weight}$;
- Deriving $L_1$ is a constant $k$ (and its value is independent of weight).

The derivative of $L_2$ is a multiplier that removes x% of the weight every time, as it gets lower and lower, the percentage is also reduced, resulting in the weight just getting smaller but never zero (ref. [Zeno](https://en.wikipedia.org/wiki/Zeno's_paradoxes#Dichotomy_paradox)).

On another hand, the derivative of $L_1$ is a subtracting constant applied to the weights. Because of the $L_1$ absolute nature, it has a discontinuity at 0, which causes subtraction that cross 0 to be zeroed out. For instance, if a weight is `0.01` and the subtraction forces it to `-0.02`, the regularization term will set it to exactly **zero**.

### Important notions

- Switching from $L_2$ to $L_1$ regularization drastically reduces the delta between test loss and training loss.
- Switching from $L_2$ to $L_1$ regularization **dampens** all the learned weights.
- Increasing the $L_1$ regularization rate generally dampens the learned weights. **However**, if the rate goes too high, the model can't converge and losses are very high.
- $L_1$ will encourage most of the non-informative weights to be exactly zero. By doing so, those features leave the model. But, it can also **reduce to zero informative weights** that: are weakly informative or strongly informative on different scales and informative features very correlated with other similarly informative features.
- For models with a great number of non-informative features, $L_1$ tends to reduce the number of them, resulting in a reduction of the model size.


&nbsp;


&nbsp;

---
## Neural Networks


---
Tags:
[[19-05-2022]], [[Curso]], [[Exerc√≠cios]]