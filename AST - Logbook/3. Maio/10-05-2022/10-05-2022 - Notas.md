# Machine Learning - Google Crash Course
## Representation with a Feature Cross - Exercise (California Housing Dataset)
On previous exercises, we trained on only 1 feature, but this one trains on 2. They will be stored in a TensorFlow proprietary structure `tf.feature_column` that allows the representation of a **single feature**, **single feature cross** or **single synthetic feature** in a desired way.

To represent a feature as floating-points values, we use [`tf.feature_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column). When we want to use bins/buckets, there's the [`tf.feature_column.bucketized_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column) option.

Note that these representations still need to be appended to a Python list.

For this exercise, we scrub the latitude and longitude columns from the .csv document to use as our features, and store them in a list:
```py
#Empty list to store all the feature columns
feature_columns = []

#Columns with latitude floating-point values 
latitude = tf.feature_column.numeric_column("latitude")
feature_columns.append(latitude)

#Columns with longitude floating-point values 
longitude = tf.feature_column.numeric_column("longitude")
feature_columns.append(longitude)

#Creation of a layer (part of the model)
fp_feature_layer = layers.DenseFeatures(feature_columns)
```

The layer will process the raw inputs according to the transformations described by the feature columns, and pack the results into a numeric array that the model, ultimately, will train on.

After training for these 2 *single* features, the results are:

<p align="center">
	<img src="https://i.imgur.com/J4VPxBh.png" width=600>
</p>

The loss is high and does not converge.

### Task 1 - Why aren't floating-point values a good way to represent latitude and longitude?
Floating-point values aren't a good choice for predicting house prices, since there's not a linear correlation between a house a latitude x and one at x+1 (the latter house is not x/x+1 more valuable). Representing latitude and longitude as floating-point values is not a good choice, we're just using the raw data which has no predictive power whatsoever. 


To better the results, let's try another approach: representing latitude and longitude in **buckets**. There will be approximately 20 bins:
- 10 for `latitude`;
- 10 for `longitude`.

The advantage is that the model will learn a weight for each different bucket. For example, all the houses that are placed in, let's say, the bucket corresponding to latitude 35 will have the same weight, but the ones in 36 have a different weight (and so on).

```py
resolution_in_degrees = 1.0

#New empty list that will store the generated columns
feature_columns = []

#Bucket column for latitude
latitude_as_a_numeric_column = tf.feature_column.numeric_column("latitude")

latitude_boundaries = list(np.arange(int(min(train_df['latitude'])),
				int(max(train_df['latitude'])),
			   	resolution_in_degrees))
			   
latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)

feature_columns.append(latitude)



#Bucket column for longitude
longitude_as_a_numeric_column = tf.feature_column.numeric_column("longitude")

longitude_boundaries = list(np.arange(int(min(train_df['longitude'])),
				int(max(train_df['longitude'])),
				resolution_in_degrees))
			
longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column,longitude_boundaries)

feature_columns.append(longitude)


#Convert the list of feature columns into a layer
buckets_feature_layer = layers.DenseFeatures(feature_columns)
```
Every value will be spaced within 1 degree of resolution. The new results are:

<p align="center">
	<img src="https://i.imgur.com/ZKaE8O9.png" width=600>
</p>
There's a clear convergence! The results are, therefore, better, but the loss is still high.

### Task 2 - Did buckets outperform floating-point representations?

Yes, performing training over bucketed data has proven better when compared with floating-point representation

### Task 3 - What is a better way to represent location?
Since there's no linear correlation, we can create it by using **feature cross**. This makes all the sense, since we started by representing locations through individual latitude and longitude values, even though, in real world, we always work in 2 dimensions. The solution is obvious, we perform feature cross between the latitude and longitude values and end up with a "grid like" location representation.

This way, the model will learn weight not for each value, not for each bin, but for each 2 dimensional cell.

To represent location as a feature cross, we use the function `tf.feature_column.crossed_column` to cross the buckets:

```py
resolution_in_degrees = 1.0

# Create a new empty list that will eventually hold the generated feature column.
feature_columns = []

(Omitted creation of bucket feature columns for latitude and longitude, since it was seen in the previous step)


# Create a feature cross of latitude and longitude.
latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)

crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)

feature_columns.append(crossed_feature)

# Convert the list of feature columns into a layer that will later be fed into the model.
feature_cross_feature_layer = layers.DenseFeatures(feature_columns)
```

The results are the following:
 
<p align="center">
	<img src="https://i.imgur.com/Qshuie4.png" width=600>
</p>

### Task 4: Did the feature cross outperform buckets?
Yes, representing the features as a feature cross presents lower loss values than the other two methods (floating-point and bins).

### Task 5: Adjust the resolution of the feature cross
A `resolution_in_degrees = 1.0` means that every cell has an area of 1.0 degrees of latitude by 1.0 degrees of longitude, that corresponds to a cell of 110 km by 90 km. If we reduce the `resolution_in_degrees`, we will have higher precision to pinpoint a specific house (instead of a whole neighborhood). 

For this case, the best value appears to be around `resolution_in_degrees = 0.4`, although, if it is too low, the loss increases since the dataset doesn't contain enough examples in each cell to predict prices in those cells.

A better feature that does not exist in the provided dataset would be the postal code, eliminating the need to perform feature cross.


---
Tags:
[[10-05-2022]], [[Curso]], [[Exerc√≠cios]]