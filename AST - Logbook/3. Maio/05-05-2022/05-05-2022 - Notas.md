# Machine Learning - Google Crash Course
## Representation: Feature Engineering
### Mapping Raw Data to Features
A feature vector comprises the examples in our data set. Feature engineering is transforming the raw data into a feature vector, since it doesn't come as one.

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/RawDataToFeatureVector.svg" width=600>
</p>

### Mapping Numeric Values
Integers and floating-point values don't need a special encoding to transform them into feature vectors because they can be directly multiplied by a numeric weight. It's a trivial transformation from, for example, a value 6 to 6.0:

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/FloatingPointFeatures.svg" width=600>
</p>

### Mapping Categorical Values
Categorical data are features that have a discrete set of possible values. For example, a feature named `street_name` will have discrete possibilities like:
```py
{'Charleston Road', 'North Shoreline Boulevard', 'Shorebird Way', 'Rengstorff Avenue'}

```

Because models **can't multiply strings by the learned weights**, we must use feature engineering to convert strings to numeric values. One way of doing it is by mapping each option in the feature (vocabulary) to an integer. All the other options that we might encounter that are not in the feature vector are classified and stored in a "catch-all" category called OOV (Out-Of-Vocabulary) bucket.

For the specified example, it would look something like this:
- map Charleston Road to 0;
- map North Shoreline Boulevard to 1;
- map Shorebird Way to 2;
- map Rengstorff Avenue to 3;
- map *everything else* (OOV) to 4.

However, this brings up a couple of problems;
- The model will learn a single weight that will be applied to every street in the vector. We need a model with flexibility that will attribute different weights to different streets to prevent labels like price.
- Cases where the house is in multiple streets (corner house) are not taken into account.

A good *solution* that solves both constraints is creating a *binary vector* called **one-hot encoding**. For each categorical feature in our model that represents values: the element is set to 1 when the value applies to the example and 0 otherwise.

The length of the binary vector is equal to the number of elements in the vocabulary. When a single value is 1 the representation is called **one-hot encoding** but when multiple values are 1 then it is **multi-hot encoding**

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/OneHotEncoding.svg" width=600>
</p>


### Sparse Representation
Suppose that we had $1.0\times10^6$ different street names that we wanted to include as values for the feature vector `street_name`. Creating a vector with that astronomical size where only 1 or 2 elements are non-zero is inefficient, both computation and time wise. For this case, a common approach is to use **sparse representation** in which only non-zero values are stored (an independent weight is still learned for each feature value).

&nbsp;  

&nbsp;  

## Representation: Qualities of Good Features
### Avoid rarely used discrete feature values
Good features should appear more than 5 times in a data set, doing so enables the model to learn how this feature value rebates to the label. Having `house_type_ victorian` is a good feature because it would show up in many examples, but `unique_house_id: 8SK982ZZ1242Z` makes no sense since it is *unique* each value would only be used once.  

### Prefer clear and obvious meanings  
Each feature should be clear and obvious. For example, `house_age_years: 27` is a good feature, but `house_age: 851472000` is not. We must also pay attention to noise data such as `user_age_years: 277`.  

### Don't mix "magic" values with actual data  
Good floating-point features don't have out of bonds values or "magic" values. Suppose a feature holds values between 0 and 1. Situations like the following are fine:
- `quality_rating: 0.81`
- `quality_rating: 0.37`

However, if a user didn't log a quality rating value, we would get `quality_rating: -1`. It indicates that a value is absent and is malpractice. The proper way to indicate that a value is missing is to create a Boolean feature specifically for that purpose: `is_quality_rating_defined`.  

The magic values should be replaced as follows:
- For discrete variables, add a new value to the set and use it to signify that the feature value is missing;
- For continuous variables, we must ensure that the missing values do not affect the model by using the mean value of the feature's data.  

### Account for upstream instability  
The definition of a feature **shouldn't** change over time. The case `city_id: "pt/coimbra"` makes sense, since the city name won't change. However, `inferred_city_cluster: "219"` currently representing Coimbra, can easily change in the future.

&nbsp;  

&nbsp;  

## Representation: Cleaning Data
### Scaling feature values
**Scaling** is converting a floating-point feature value from its natural range (900 to 1000) to a more manageable, standard range (0 to 1 or -1 to +1). For a feature set of just one feature, there's no practical benefit. However, if a feature set consists of multiple features, then feature scaling has the following vantages:
- The gradient descent converges more quickly;
- The "NaN trap" (Not a Number), in which one number in the model becomes a NaN and due to math operations every other number eventually becomes a NaN, can be more easily avoided;
- Helps the model learn appropriate weights for each feature. Without feature scaling, the model will pay too much attention to features with a wider range.

#### Z score scaling
One of the most obvious ways to scale numerical data is to linearly map $[min, max$] to a small scale such as $[-1, +1]$. 

Another popular scaling technique is to calculate the Z score of each value (it relates the number of std deviations away from the mean).
$$
val_{scaled}=\frac{\text{value}-\text{mean}}{\text{std dev}}
$$
Most scaled values will be between -3 and +3, but a few others will be a little higher or lower than that range.


### Handling extreme outliers  
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/ScalingNoticingOutliers.svg" width=600>
</p>


The previous graphic shows the proficiency of rooms per person in California. It has a very long tail with minimal information that end up influencing the result. One way to minimize the influence of these extreme outliers is to change the scale to a log scale:  

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/ScalingLogNormalization.svg" width=600>
</p>  

With a logarithmic scale, the tail is smaller, but still significant. Another approach to solve the problem is **clipping** the feature values at an arbitrary value, let's say 4.0.

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/ScalingClipping.svg" width=600>
</p>

Clipping the values at 4.0 doesn't mean that all that is greater than that is ignored. Rather, it means that all the values greater than 4.0 will now become 4.0 instead (which explains the small bump).  
  
  
### Binning
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/ScalingBinningPart1.svg" width=600>
</p> 

The previous plot shows the prevalence of houses related to the latitude. Los Angeles is about at latitude 34 and San Francisco is roughly at 38, there's an evident cluster pattern.

In our data set, latitude is a floating-point. However, there's no sense in representing it as a floating-point in our house prediction model, because there's no linear relation between the latitude number and the house value. And yet, individual latitudes probably are a good prediction of house values because it allows the creation of region clusters.

To make latitude a helpful price predictor, let's divide it into **bins**:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/ScalingBinningPart2.svg" width=600>
</p> 

Instead of just one floating-point feature, we now have 11 boolean features (`LatitudeBin1`, `LatitudeBin2`, ..., `LatitudeBin11`) that will be united into a single 11 elements vector. Latitude 37.4 is represented as follows:

```py
[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
```

Thanks to binning, our model can now learn completely different weights for each latitude.  

### Scrubbing
Up until this point, we assumed that all the data used for training and testing was trustworthy. In real life, many examples in data sets are unreliable due to one (or more) of the following reasons:
- **Omitted values** -> For instance, a person forgot to log a value related to the house's age.
- **Duplicate examples**
- **Bad labels** -> Human error can lead to a mislabeling of an example.
- **Bad feature values** -> Someone can type an extra digit, or a thermometer was left out in the sun.

Once some error is detected, the bad exampled is fixed by removing them from the data set. An efficient way to detect duplicates or omitted values is to program a script for that purpose. Detecting bad feature values or labels is far more complicated because of the nature of the proposed problem.

In addition to detecting bad **individual** examples, we must also detect bad data in an aggregate. Histograms and statistics metrics (maximum and minimum, mean and median, standard deviation) are great mechanisms to visualize the data in the aggregate.


---
Tags:
[[05-05-2022]], [[Curso]]