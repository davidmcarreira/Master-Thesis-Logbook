# Machine Learning - Google Crash Course
## Regularization for Simplicity: Playground Exercise (Overcrossing?)
Sometimes it's important to not use a model that's too complicated, with too many feature crosses, for example. For noisy data, using too many feature crosses can present itself as a disadvantage, as we concur on the risk of fitting the training data to that noise, making the model perform badly on the test set. The key-word is **simplicity**.

## Regularization for simplicity: $L_2$ Regularization
Let's observe the following **[[19-03-2022 - Notas#Generalization|generalization]] curve** (loss curve showing both training and validation set):
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/RegularizationTwoLossFunctions.svg" width=600>
</p>

The figure shows the progress of loss through iterations, and as expected the Training Data loss gets lower and converges. However, we can observe an unexpected behavior in respect to the Validation Data loss: it bottoms out then increases. This indicates that it's occurring **overfitting**, the opposite of the wanted generalization. To counter that, there are some ways:
- Early stop -> stopping the training at the bottom of the validation data curve loss;
- $L_2$ generalization (ridge regularization) -> penalizing model complexity.

In other words, instead of aiming to minimize loss (empirical risk minimization) like has been done until now:
$$
min(Loss(Data|Model))
$$

We'll now have an additional parameter to penalize complexity, in order to minimize both loss **and** complexity. This process is called **structural risk minimization**:
$$
min(Loss(Data|Model) + complexity(Model))
$$

The optimization algorithm now will depend on 2 terms: the **loss term** (data dependent), which measures how well the model fits the data; and the **regularization term** (not data dependent), responsible for measuring the model complexity.

Model complexity can be viewed in 2 different functional ways:
- Higher absolute values of weight, more complexity is present;
- Higher total number of features with nonzero weights, more complexity is present.

We can **quantify complexity** using the $L_2 \text{ regularization formula}$:
$$
L_2\text{ regularization term} = ||w||^2_2 = \sum_i^n w_i^2
$$

Weights close to zero have almost no effect, while outlier weights can have a huge impact!


## Regularization for simplicity: Lambda
In order to tune the **impact of the regularization term**, there needs to be added a multiplying scalar that dictates said impact. It is usually called **regularization rate**, also known as **lambda**, $\lambda$:
$$
min(Loss(Data|Model) + \lambda \times \sum_i^n w_i^2)
$$

The lambda term is a coefficient that will tell how much we care about getting the examples right versus having a simple model.

Performing $L_2$ regularization has the following effects on a model:
- Encourages weights toward 0 (but NEVER exactly zero);
- Encourages the mean of the weights toward 0, with a normal shaped distribution (centered in zero).

The choice of the **lambda** value will depend on our data:
- If we have lots of training data and the test data looks the same (i.i.d, that is, independent and identically distributed data -> already normally distributed) then we don't need a lot of regularization => small lambda => complex model;
- If we don't have much training data or if the training data and test data look different, the distribution is all over the place, and then we might need a lot of regularization => big lambda => simple model.


The histogram of heights for a **high value** of $\lambda$:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/HighLambda.svg" width=600>
</p>

For a **small value** $\lambda$:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/LowLambda.svg" width=600>
</p>

When the $complexity(Model)$ term is big, that means the model is complex, and we risk **overfitting**, that means we need to lower it => small lambda value. The model would fit too much to the particularities of the data and wouldn't be able to generalize.

When the $complexity(Model)$ term is small, that means the model is simple, meaning that we run the risk of **underfitting** the data, so we need to increase it => big lambda value. The model wouldn't be capable of properly learning the data in order to make useful predictions.

### Early stopping
Strong $L_2$ regularization values tend to drive feature weights closer to 0. Lower learning rates combined with early stopping produce the same effects. Consequently, tweaking learning rate and lambda at the same time might lead to confusing results.

Early stopping is the act of ending training before the model fully converges. In continuous (online) training, we always end up with some type of early stopping, since some new trends haven't had enough data to converge.

The effects from changes to regularization parameters can be confounded with changes to learning rate (or even number of iterations). One useful practice is to give a high enough number of iterations that early stopping doesn't have a significant impact.

&nbsp;

-> $L_2$ regularization encourages weights to be 0, but not exactly 0.
-> $L_2$ regularization will force the features towards roughly equivalent weights.
-> Sometimes $L_2$ regularization can cause the model to learn a moderate weight for non-informative features (if said feature is correlated with the label, the model can give "credit" to the incorrect feature instead of an informative feature).


---
Tags:
[[11-05-2022]], [[Curso]]