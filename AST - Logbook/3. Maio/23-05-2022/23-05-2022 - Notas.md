
# Machine Learning - Google Crash Course
## Multi-Class Neural Networks: One vs All
Earlier, it was studied binary classifiers that only worked in a single class (spam/not spam or benign/malign), but there are cases where we need more. For example, if we want to identify a breed of a dog, we must have multiple classes related to each breed that we are studying. Real world problems can have *millions* of different classes.

In a classification problem with N possible outcomes, a **one vs all** solution provides N separate binary classifiers (one for each solution). During training, the model goes through each classifier and trains them to answer a separate classification question.

For instance, in a dog classification scenario, a picture is fed to the model in order to recognize if it is a dog or not. We can train, for example, 5 classifiers:
1. Is this image an apple? Yes/No
2. Is this image a bear? Yes/No
3. Is this image a candy? Yes/No
4. Is this image a dog? Yes/No
5. Is this image an egg? Yes/No

Obviously, only one classifier will output **True** as a result, the other four will classify it a negative example.

When the total number of classes is small, this approach is not bad, but when the number increases, it might not be the best solution.

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/OneVsAll.svg" width=500>
</p>

&nbsp;

&nbsp;

## Multi-Class Neural Networks: Softmax
Previously, it has been seen that in **logistics regression** the output can be seen as the probability of the outcome, so the sum of results must always be one (if an email has a 0.8 score of being spam, it means that it has a 0.2 score of not being spam).

This train of thought can be extended to a multi-class situation through a **Softmax layer**. It assigns decimal probabilities to each class and those must always sum to one, helping on a more swift training convergence. 

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/SoftmaxLayer.svg" width=500>
</p>

If we look again at the previous dog example, the Softmax results would be something similar to the following:

|Class:|Probability|
|---|--:| 
|apple|0.001|
|bear|0.04|
|candy|0.008|
|dog|0.95|
|egg|0.001|

### Softmax Options
There are **2 variants** of Softmax:
- **Full Softmax** → calculates the probability for **every possible class**. It is a fairly cheap option when the number of classes is reasonable.

- **Candidate Sampling** → calculates the probability for all the positive labels, but **only for a random sample of negative labels**. For example, if we want to classify the breed of a dog, we do not have to calculate the probabilities for every non-dog related class. It improves efficiency in problems with a bigger amount of classes.


### One Label vs Many Labels
Softmax assumes that each example **only belongs to one class**, however, that is not always the case, as an example can belong to more than one class at the same time. In that case, **Softmax is not allowed** and **multiple logistic regressions must be used**.



&nbsp;

&nbsp;

## Multi-Class Classification

This exercise will use the MNIST dataset that contains thousands of handwritten numbers examples. The distribution is:
- 60000 training examples;
- 10000 test examples.

The examples consist of a 28x14 pixel map with values ranging from `0` (white) to `255` (black) that will be normalized.

<p align="center">
	<img src="https://www.tensorflow.org/images/MNIST-Matrix.png" width=650>
</p>

### Loading the dataset

The `tf.keras` method provides a set of functions to load well-known dataset such the one we are using:
```py
#Loading the MNIST dataset (already suffled)
(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()
```

- `x_train` contains the training set's features;
- `y_train` contains the training set's labels;
- `x_test` contains the test set's features;
- `y_test` contains the test set's labels;

To print an example, all that is need to do is:
```py
# Output example #2917 of the training set.
x_train[2917]
```
The output is an array of integers that can be interpreted as an image by using `matplotlib.pyplot.imshow`:
```py
# Use false colors to visualize the array.
plt.imshow(x_train[2917])
```

<p align="center">
	<img src="https://i.ibb.co/HhmBp5v/download.png" width=400>
</p>

### Task 1: Normalize feature values
```py
#The values range from 0 to 255
x_train_normalized = x_train/255
x_test_normalized = x_test/255
print(x_train_normalized[2900][10]) #Output a normalized row
```


### Method to create a deep neural net model

The `create_model` routine defines the topography of the deep neural net, it specifies:
- The number of layers;
- The activation function for each layer;
- The number of nodes in each layer;
- Regularization layers (when needed).

```py
def create_model(my_learning_rate):
	# All models in this course are sequential.
	model = tf.keras.models.Sequential()

	#Flattening the 28x28 feature array
	model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
	
	#First layer
	model.add(tf.keras.layers.Dense(units=32, activation='relu'))
	#Dropout (regularization) layer
	model.add(tf.keras.layers.Dropout(rate=0.2))
	#Output layer
	model.add(tf.keras.layers.Dense(units=10, activation='softmax'))
	
	#Contruction of the layers into a model 
	#(now the loss function is different)
	model.compile(optimizer=
		tf.keras.optimizers.Adam(lr=my_learning_rate),
		loss="sparse_categorical_crossentropy",
		metrics=['accuracy'])
		
	return model
```

It is important to notice that the output layer **must have 10 nodes** since we are evaluating digits from 0 to 9, other value than that is wrong.

### Method to train the model
On the contrary to previous exercises, no feature columns or feature layer are defined. Instead, the model will train directly on the `NumPy` array.

```py
def train_model(model, train_features, train_label, epochs,
				batch_size=None, validation_split=0.1):
				
	history = model.fit(x=train_features,
						y=train_label,batch_size=batch_size,
						epochs=epochs, shuffle=True,
						validation_split=validation_split)


	#Snapshot of the model's metrics at each epoch
	#to track the progression of training
	epochs = history.epoch
	hist = pd.DataFrame(history.history)
	
	return epochs, hist
```

### Invoking the previous functions
Now for the actual training of the model on the training set and further evaluation against the test set:
```py
#Hyperparameters
learning_rate = 0.003
epochs = 50
batch_size = 4000
validation_split = 0.2

#Model topography
my_model = create_model(learning_rate)

#Training on the normalized test set
epochs, hist = train_model(my_model, x_train_normalized, y_train,
						   epochs, batch_size, validation_split)
						   
#Plot metrics vs epochs
list_of_metrics_to_plot = ['accuracy']
plot_curve(epochs, hist, list_of_metrics_to_plot)

#Evaluating against the test set
my_model.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)
```

The plot of *Accuracy vs Epoch*:
<p align="center">
	<img src="https://i.ibb.co/422Hprz/download.png" width=500>
</p>

The model achieved a loss of `0.1412` and accuracy of `0.9596` and, when compared to the test set, the loss is ` 0.1394` and accuracy is `0.9579`.

### Task 2: Optimize the model
Here is what we discovered:
- Adding more nodes (at least until 256 nodes) to the first hidden layer improved accuracy.
- Adding a second hidden layer generally improved accuracy.
- When the model contains a lot of nodes, the model overfits unless the dropout rate is at least 0.5.

&nbsp;

We reached 98% test accuracy with the following configuration:
- One hidden layer of 256 nodes; no second hidden layer.
- Dropout regularization rate of 0.4

&nbsp;

We reached 98.2% test accuracy with the following configuration:
- First hidden layer of 256 nodes; second hidden layer of 128 nodes.
- Dropout regularization rate of 0.2

---
Tags:

[[23-05-2022]], [[Curso]], [[Exercícios]]