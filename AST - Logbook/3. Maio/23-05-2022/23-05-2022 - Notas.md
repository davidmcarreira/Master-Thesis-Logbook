
# Machine Learning - Google Crash Course
## Multi-Class Neural Networks: One vs All
Earlier, it was studied binary classifiers that only worked in a single class (spam/not spam or benign/malign), but there are cases where we need more. For example, if we want to identify a breed of a dog, we must have multiple classes related to each breed that we are studying. Real world problems can have *millions* of different classes.

In a classification problem with N possible outcomes, a **one vs all** solution provides N separate binary classifiers (one for each solution). During training, the model goes through each classifier and trains them to answer a separate classification question.

For instance, in a dog classification scenario, a picture is fed to the model in order to recognize if it is a dog or not. We can train, for example, 5 classifiers:
1. Is this image an apple? Yes/No
2. Is this image a bear? Yes/No
3. Is this image a candy? Yes/No
4. Is this image a dog? Yes/No
5. Is this image an egg? Yes/No

Obviously, only one classifier will output **True** as a result, the other four will classify it a negative example.

When the total number of classes is small, this approach is not bad, but when the number increases, it might not be the best solution.

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/OneVsAll.svg" width=500>
</p>

&nbsp;

&nbsp;

## Multi-Class Neural Networks: Softmax
Previously, it has been seen that in **logistics regression** the output can be seen as the probability of the outcome, so the sum of results must always be one (if an email has a 0.8 score of being spam, it means that it has a 0.2 score of not being spam).

This train of thought can be extended to a multi-class situation through a **Softmax layer**. It assigns decimal probabilities to each class and those must always sum to one, helping on a more swift training convergence. 

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/SoftmaxLayer.svg" width=500>
</p>

If we look again at the previous dog example, the Softmax results would be something similar to the following:

|Class:|Probability|
|---|--:| 
|apple|0.001|
|bear|0.04|
|candy|0.008|
|dog|0.95|
|egg|0.001|

### Softmax Options
There are **2 variants** of Softmax:
- **Full Softmax** → calculates the probability for **every possible class**. It is a fairly cheap option when the number of classes is reasonable.

- **Candidate Sampling** → calculates the probability for all the positive labels, but **only for a random sample of negative labels**. For example, if we want to classify the breed of a dog, we do not have to calculate the probabilities for every non-dog related class. It improves efficiency in problems with a bigger amount of classes.


### One Label vs Many Labels
Softmax assumes that each example **only belongs to one class**, however, that is not always the case, as an example can belong to more than one class at the same time. In that case, **Softmax is not allowed** and **multiple logistic regressions must be used**.









---
Tags:

[[23-05-2022]], [[Curso]], [[Exercícios]]