# Reunião 06/05/2022
# Tópicos discutidos:
Fazer resumo em ppt do trabalho:
- O que fiz;
- Dificuldades;
- Objetivos cumpridos;
- Trabalho proposto para próxima semana;

Enviar o estado de arte.

---

# Machine Learning - Google Crash Course
## Feature Crosses: Encoding Nonlinearity
For a previous studied problem, is easy to define a model that spearates the data with just a line (linear model):

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/LinearProblem1.png" width=300>
</p>

 But what is the data is slightly different?
 <p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/LinearProblem2.png" width=300>
</p>

This is not a linear problem, since we can't draw a line that separates the blue dots from the orange ones without having very bad results. A solution for the nonlinearity shown in the previous figure is to create a **synthetic feature** through a method called **feature cross**. A **feature cross** encodes nonlinearity in the feature space by multiplying two or more input features together (cross comes from the *cross product*).

Let's create a feature cross $x_3$:
$$
x_3=x_1x_2
$$

This newly minted feature is treated just like the others. The linear formula becomes:
$$
y=b+w_1x_1+w_2x_2+w_3x_3
$$

A linear algorithm can learn a weight for $w_3$ just as it would for $w_1$ or $w_2$, in other words $w_3$ encodes nonlinear information, we don't need to change how the linear model trains to determine the value of $w_3$.

### Kinds of feature crosses
We can create many types of feature crosses:
- $[A \times B]$: produces a feature cross by multiplying the values of two features:
- $[A \times B \times C \times D \times E]$: produces a feature cross formed by multiplying the values of 5 features.
- $[A \times A]$: produces a feature cross by squaring a single feature.

Thanks to stochastic gradient descent, linear models can be trained very efficiently. Consequently, supplementing scaled linear models with feature crosses has traditionally been an efficient way to train on **massive-scale data sets**.

## Feature Crosses: Crossing One-Hot Vectors
Until now, we've been crossing floating-point features. But in reality, hot-one vectors are, obviously, widely used in machine learning, so their feature crossing is common. The results are logical conjunctions, such as:
```py
[country X language] => country:usa AND language:spanish
```

In another example, suppose that we bin latitude and longitude, producing separate 5 element one-hot feature vectors:
```py
binned_latitude = [0, 0, 0, 1, 0]
binned_longitude = [0, 1, 0, 0, 0]
```

If we create a cross of these 2 features:
```py
binned_latitude X binned_longitude
```

We get a 25 elements one-hot vector (24 zeroes and 1 one). The single 1 in the cross identifies a particular conjunction of latitude and longitude. The model can then learn particular associations about that specific conjunction.

If the latitude and longitude are binned as follows:
```py
binned_latitude(lat) = [
  0  < lat <= 10
  10 < lat <= 20
  20 < lat <= 30
]

binned_longitude(lon) = [
  0  < lon <= 15
  15 < lon <= 30
]

```

The resulting feature cross produces the following synthetic feature:
```py
binned_latitude_X_longitude(lat, lon) = [
  0  < lat <= 10 AND 0  < lon <= 15
  0  < lat <= 10 AND 15 < lon <= 30
  10 < lat <= 20 AND 0  < lon <= 15
  10 < lat <= 20 AND 15 < lon <= 30
  20 < lat <= 30 AND 0  < lon <= 15
  20 < lat <= 30 AND 15 < lon <= 30
]
```


Another great example is a model that predicts how satisfied dog owners will be with dogs based on 2 features:
- Behavior type (barking, crying, snuggling, etc);
- Time of day.

If we build a feature cross from both these features:
```
[behavior type X time of day]
```

We end up with vastly more predictive ability than either feature on its own. For example, if a dog cries, it can be from happiness or sadness. That information can be correlated with the time of the day (if it's 5:00pm when the owner comes from work, then it will be a "happy" cry; if it's 3:00am when the owner is sleeping, then it indicates it is a "sad" cry), leading to a prediction of owner satisfaction.

Linear learners scale **very well to massive data**. Using feature crosses on massive data is one efficient strategy for learning highly complex models. Another way is by using **neural networks**.





---
Tags:
[[06-05-2022]], [[Reunião]], [[Curso]], [[Exercícios]]