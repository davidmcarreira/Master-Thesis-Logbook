# Machine Learning - Google Crash Course
## Logistic Regression: Calculating a Probability
Linear regression makes a continuous prediction of exactly a value, **logistic regression**, on the other hand, gives us a **probability** value. This model is used in cases where the predicted value might lay outside the 0 to 1 range, but we need a probability output.

Logistic regression will give us calibrated probabilities that can be used afterwards to obtain, for example, expectation values in classification (discrete predictions) tasks where we want to know if a certain prediction is either x or y.

To ensure that the results are always between 0 and 1, we take the usual linear regression model and feed it into a **sigmoid function**:
$$
y'=\frac{1}{1+e^{-z}}
$$
where $y'$ is the predicted value for a particular example and $z = b+w_1x_1+w_2x_2+...+w_nx_x$ is the linear regression model. The sigmoid function is:

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/LogisticRegressionOutput.svg" width=600>
</p>

And as we can see, the "Probability Output" is forced to stay between 0 and 1 (but never exactly, as they are asymptote values).

The $z$ value is usually also called *log-odds* because the inverse of the sigmoid gives the logarithm of the probability of 1, divided by the probability of not occurring 1 (i.e, 0):
$$
z=log\left(\frac{y}{1-y}\right)
$$

### Logistic Regression Example
For the following bias and weights:
- $b = 1$
- $w_1 = 2$
- $w_2 = -1$
- $w_3 = 5$

And the following feature values for a given example:
- $x_1 = 0$
- $x_2 = 10$
- $x_3 = 2$

Our $z$ is: $z = b+w_1x_1+w_2x_2+w_3x_3 = 1+2\times0+(-1)\times10+5\times2 = 1$

Therefore, the logistic regression prediction for this example will be:
$$
y'=\frac{1}{1+e^{-1}}=0.731
$$

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/LogisticRegressionOutput0.731.svg" width=400>
</p>



## Logistic Regression: Loss and Regularization

The loss function for this model will also be different from the squared loss in linear regression. For **Logistic Regression** the calculus of the loss is through the **Log Loss**:
$$
L_{log}=\sum_{(x, y)\epsilon D} -ylog(y')-(1-y)log(1-y')
$$

where:
- $(x, y)\epsilon D$ is the data set containing labeled examples pairs;
- $y$ is the label (in this case 0 or 1);
- $y'$ is the predicted value (also between 0 and 1) for a given set of features in x.

The loss function has the following graphic:

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/LoglossDefined.svg" width=400>
</p>


## Regularization in Logistic Regression

The asymptotic nature of the model might cause a problem if regularization is not included. As the model might try to bring the loss to zero, and because we have asymptotes for both extremes, the model would never reach the desired values, hence the weights increasing too much. Once again, the solution for this problem would be **[[11-05-2022 - Notas#Regularization for simplicity L_2 Regularization|regularization]]** (or early stopping).

For example, if we assigned a unique id to each example, map each id to its own feature and didn't specify any regularization function, the model would totally overfit. It would try to drive the loss to zero on every single one of the examples and never get there, leading to an increase in the weights for each feature to infinity (minus and/or plus). This is common in high dimensional data with feature crosses, when there's an enormous mass of rare crosses that only happen once for each example.





---
Tags:
[[12-05-2022]], [[Curso]]