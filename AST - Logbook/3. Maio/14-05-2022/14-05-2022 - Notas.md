# Machine Learning - Google Crash Course
## Classification: Thresholding
Like previously mentioned, logistic regression returns a probability, while classification returns a discrete output. Although we can use the returned probability "as is", it can also be converted to a binary value if a threshold is imposed. 

For example, let's consider a spam detection model that has the objective of saying if an email is legitimate or not. The data is run through a logistic regression model, and it gives us 0.9995 for a specific message, so it is almost most certainly **spam**. Conversely, another message generates 0.0003 and is very likely **not spam**. These results are not dubious, so it's easy to place them in the spam or not spam category. However, for predictions of, let's say, 0.6 or 0.55, it's much harder to identify the nature of the email. 

Therefore, in order to convert a logistic regression value to a binary category, a **classification threshold** (also called decision threshold) must be implemented, where a value above that is classified as spam whereas one below indicates that it is not spam.

Intuitively, the threshold might be set to 0.5, but that's not always the case as it is always problem dependent.

Tuning a threshold is very different that tuning the model hyperparameters. Choosing the threshold is assessing how bad are the consequences of a bad result (classification a "not spam" email as "spam" is grave, but classification a "spam" email as "not spam" is not the end of the world). 

&nbsp;

## Classification: True vs False and Positive vs Negative

Definitions:
- "Wolf" is a **positive class**;
- "No wolf" is a **negative class**;

This generates 4 hypotheses:
1. Call for wolf and there's a wolf -> True Positive (TP)
2. Call for wolf and there's no wolf -> False Positive (FP)
3. No call for wolf	and there's a wolf -> False Negative (FN)
4. No call for wolf and there's no wolf -> True Negative (TN)

A **true positive** is when an outcome of the model *correctly* predicts the **positive class**. A **true negative** is when the model *correctly* predicts the **negative class**.

A **false positive** is when the prediction of the model *incorrectly* predicts the **positive class**. A **false negative** is when the model *incorrectly* predicts the **negative class**.

&nbsp;

## Classification: Accuracy

Accuracy is a *metric* that measures of how close to the correct result we are and is used to **evaluate classification models**.
$$
\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{{Total number of predictions}}}
$$

It's the fraction of predictions our model got right.

For binary classifications, it can be calculated in terms of positives and negatives:
$$
\text{Accuracy}=\frac{TP+TN}{TP+TN+FP+FN}
$$

Example:
$$
\text{Accuracy}= \frac{TP+TN}{TP+TN+FP+FN} = \frac{1+90}{1+90+1+8} = 0.91
$$

The accuracy is 91% (91 correct predictions out of 100 examples). On a first instance, we can assume that the model is doing a good job, but it might not always be the case. If this was a tumor classifier (*benign* is negative and *malignant* is positive), out of 100 tumor examples, 91 are benign (90 TN and 1 FP) and 9 are malignant (1 TP and 8 FN). The model would identify correctly 90 tumors as benign, however, of the 9 malignant tumors, only 1 is correctly identified and the other 8 go undiagnosed. These are bad results, as there as attributed a benign classification to a malignant!

Accuracy alone is not a fool-proof measure, specially in cases where we have class-imbalanced data sets (like this example, where there's a significant disparity between the number of positive and negative labels or the "ad clicking" example, where only 1 in 1000 or 1 in 10000 are positives).

&nbsp;


## Classification: Precision and Recall
### Precision  
Precision is a metric that measures how close to each other the results are. In an ML context, it is also interpreted as the **proportion between correct positive identifications and all positive predictions**.
$$
\text{Precision} = \frac{TP}{TP+FP}
$$

A model that produces no false positive has a precision of 1.0!

Example:
For the following distribution:  
- TP: 1
- FP: 1
- FN: 8
- TN: 90

The accuracy is:
$$
\text{Precision} = \frac{TP}{TP+FP} = \frac{1}{1+1} = 0.5
$$
In other words, when it predicts a tumor is malignant, it is correct 50% of the time.

### Recall
Recall is a metric that measures the **proportion between correct predicted positives and all the available positives** (TP and FN):
$$
\text{Recall} = \frac{TP}{TP+FN}
$$

Example:
For the following distribution:  
- TP: 1
- FP: 1
- FN: 8
- TN: 90

The recall value is:
$$
\text{Recall} = \frac{TP}{TP+FN} = \frac{1}{1+8} = 0.11
$$

The model correctly identifies 11% of all the malignant tumors.

## Precision and Recall: A Tug of war
The effectiveness of a model is dictated by both the precision and recall, as they are often in tension between each other. Improving precision typically reduces recall and vice-versa.
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/PrecisionVsRecallBase.svg" width=700>
</p>
The "dots" at the right of the classification threshold are classified as "spam" emails, and the ones on the left are "not spam".

The classification distribution is:
- TP: 8
- FP: 2
- FN: 3
- TN: 17

**Precision** will measure the percentage of emails **flagged as spam** that were correctly classified (the percentage of dots placed to the right of the threshold):
$$
\text{Precision} = \frac{TP}{TP+FP} = \frac{8}{8+2} = 0.80
$$

**Recall** measures the percentage of **actual spam emails** that were correctly classified (the percentage of green dots that are to the right of the threshold line relative to all the existing green dots):
$$
\text{Recall} = \frac{TP}{TP+FN} = \frac{8}{8+3} = 0.73
$$

&nbsp;

If the **classification threshold** is **increased**:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/PrecisionVsRecallRaiseThreshold.svg" width=700>
</p>

The prediction distribution will change as follows:
- TP: 7
- FP: 1
- FN: 4
- TN: 18
$$
\text{Precision} = \frac{TP}{TP+FP} = \frac{7}{7+1} = 0.88
$$
$$
\text{Recall} = \frac{TP}{TP+FN} = \frac{7}{7+4} = 0.64
$$

&nbsp;

Whereas, if the **classification threshold** is **decreased**:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/PrecisionVsRecallLowerThreshold.svg" width=700>
</p>

- TP: 9
- FP: 3
- FN: 2
- TN: 16

$$
\text{Precision} = \frac{TP}{TP+FP} = \frac{9}{9+3} = 0.75
$$
$$
\text{Recall} = \frac{TP}{TP+FN} = \frac{9}{9+2} = 0.82
$$


Precision and Recall is a trade-off. When we increase the threshold, we are, most likely, reducing the False Positives and True Positives and increasing the False Negatives, so the Precision increases and the Recall reduces. 

Whereas, when the threshold is reduced, there's a higher chance that the False Positives and True Positives increase, and the False Negatives and True Negatives decrease. This way the Precision decreases and the Recall increases.

&nbsp;

## Classification: ROC Curve and AUC
Another way of evaluating the performance of a model is by using the ROC Curve and the AUC.

### ROC Curve
A ROC Curve (Receiver Operating Characteristic) is a graph that shows the **performance** of a classification model at **all thresholds**. The curve plots 2 parameters:
- True Positive Rate;
- False Positive Rate

**True Positive Rate** (TPR) is the same as **recall**, so is defined as:
$$
\text{TPR} = \frac{TP}{TP+FN}
$$

And **False Positive Rate** (FPR) is defined as:
$$
\text{FPR} = \frac{FP}{FP+TN}
$$

The plot of TPR vs FPR occurs at different classification thresholds. These parameters are calculated for all (or many) classification thresholds, and the plot line is drawn based on that. 

Lowering the threshold classifies more examples as positive, thus increasing both FP and TP. The typical curve looks like the following:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/ROCCurve.svg" width=600>
</p>

### AUC: Area Under the ROC Curve
AUC stands for **Area Under the ROC Curve**, and measures the entire 2D area underneath the ROC curve from (0,0) to (1,1).
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/AUC.svg" width=600>
</p>

The AUC provides a measure of performance across all possible thresholds. One way of interpreting AUC is the probability that the model ranks a **positive** example higher than a **negative** example. For example:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/AUCPredictionsRanked.svg" width=600>
</p>
AUC represents the probability that a green (positive) is positioned to the right of a red (negative) example. The values range from 0 to 1: an AUC of 0.0 means that it is 100% **wrong**; an AUC of 1.0 means that the model is 100% **correct**.

AUC is desirably for 2 reasons:
- It's **scale-invariant** (measures how well predictions are ranked, rather than their absolute values);
- It's **classification-threshold-invariant** (measures the quality of the model's prediction irrespective of what classification threshold is chosen).

However, both these reasons come with caveats that limit its use in certain situations:
- Scale invariance is not always desirable. Sometimes we need well calibrated probability outputs, and AUC won't tell us that.
- Classification-threshold invariance. When there are wide disparities in the cost of false negatives vs false positives, it may be critical to minimize one type of classification error. For example, in spam detection, we might want to minimize false positives.

&nbsp;

## Classification: Prediction Bias
The foundations of logistic regression predictions is that they should be unbiased, that is, the average of predictions should be roughly the same as the average of observations.

**Prediction bias** is the quantity of how far apart those two averages are.
$$
\text{prediction bias} = \text{average of predictions} - \text{average of lavels in data set}
$$

A significant nonzero prediction bias is an indication of a bug, as it leads us to believe that the model might be wrong about how frequently positive labels occur.

For example, if we know that, on average, 1% of every email is spam, and we don't know anything about a certain email, then we can predict that it's 1% likely spam. A good model should also be able to do that prediction (the average of the predictions of each individual email being spam, should be 1%). However, if the model outputs a 20% likelihood of being spam, then it exhibits **prediction bias**.

Possible causes:
- Incomplete feature set;
- Noisy data set;
- Buggy pipeline (set of data processing elements where the output of one is the input of the next one);
- Biased training sample;
- Overly strong regularization.

To correct this problem, a calibration layer can be used where, for example, if we have a 3% bias, the layer would lower the mean prediction by said 3%. However, adding a calibration layer might not be a good idea, because:

- The cause is still there;
- It's hard to maintain a system with those layers up to date.

A good model doesn't necessarily have zero prediction bias, it would be *near* zero, but exactly zero is probably an indication that there's a bug or the model is bad.

### Bucketing and Prediction Bias
Logistics regression prediction only outputs a value between **zero** and **one**. However, all the labeled examples are either **exactly** zero or one, respectively, "not spam" or "spam". 

When the prediction bias is being evaluated, it can't be predicted for just only one example, it needs to be created "buckets" of examples. The prediction bias for a logistics regression only makes sense when enough examples are grouped together to compare a **predicted value** (for example, 0.392) to **observed values** (0.394).

The following calibration plot is for a particular model. Each dot represents 1000 values per bucket. The **x-axis** is the **average of values the model predicted**, and the **y-axis** is the **actual average in the data set**.

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/BucketingBias.svg" width=600>
</p>

Part of the model performs very poorly, that can be because of the following:
- The training set doesn't properly represent certain subsets of the data space;
- Some subsets can be noisier;
- The model is very regularized (the lambda value is too high), i.e, too simplified.




---
Tags:
[[14-05-2022]], [[Curso]], [[Exercícios]]