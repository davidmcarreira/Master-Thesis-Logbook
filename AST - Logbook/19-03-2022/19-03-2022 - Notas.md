# Machine Learning - Google Crash Course
## Generalization
The ability that the model has to adapt properly to new, unseen data, drawn from the same distribution as the one used to create the model.

### Peril of Overfitting

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/GeneralizationA.png" width=400>
</p>
The dots in the image correspond to labeled data. They can be distributed as:
- Orange dots are healthy trees;
- Blue dots are sick tress.

If we think about a model to differentiate between classes of data by predicting what is what, our mind would probably divide it with a diagonal line almost in the middle. But that would incur in some kind of loss. That being said, a more complex model can be engineered to give us almost zero loss:

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/GeneralizationB.png" width=400>
</p>

By the looks of it, it is a great model that should yield great results. But the problem is: it was trained for the plotted data. What happens when we feed it new unlabeled data?
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/GeneralizationC.png" width=400>
</p>

As we can see, the model adapted very poorly to the new data. This behavior is called **overfitting** the peculiarities of the data it trained on. Models that are overfitted present low loss for the set it trained on and high loss for a test set, resulting in an unreliable model that can't be trusted by any means. The cause of overfitting is clearly because of making a model **more complex than necessary**. The line right through the middle probably wasn't a bad idea after all. Fundamentally, a machine learning model should land between a very well-fitted data but in the simplest possible manner.

<br>


In modern times, the fields of statistical learning theory and computational leaning theory, formalized the thoughts of a 14th century philosopher, William of Ockham. His Ockham's razor in machine learning terms can be described as:

â†’ The less complex a Machine Learning model, the more likely that a good empirical result is not just due to the peculiarities of the sample.

The latter modern field have developed **generalization bounds** (a statistical description of the model's ability to adapt to new data). They're based on factors such as:
- The complexity of the model itself;
- The model's performance on training data.


The big, main issue in a machine learning problem is to make predictions on real data, drawn from an unknown true probability distribution. Unfortunately, the model can't see the whole truth, as it can only sample from a predefined data set. That's why so important to understand if the model is overfitted and able to make proper predictions.

A model will make predictions on new unseen data, but we only have available data sets. One way to get that unseen data is to divide the data set in 2 groups:
- **Training set** - a subset used to train the model.
- **Test set** - a subset that tests the model.

A **good performance** on the test set is a great metric of good performance on new data, in general - provided that some assumptions are made:
- The test set is **large** enough;
- The test set is **not repeated** over iterating tests.

<br>
<br>

### The Machine Learning "fine print"

There are 3 basic assumptions:
1. The examples are drawn **independently and identically (i.i.d)** at random from the distribution. This means that the examples don't influence each other, since they have the same probability of being picked and are mutually independent.
2. The probability distribution is **stationary**, i.e, the distribution doesn't change within the data set over time.
3. The examples are drawn from partitions from the **same distribution**.

Nonetheless, sometimes these assumptions are violated. Let's see some examples:
- If a model that chooses ads to display bases its choices on what ads the user has previously seen, then the **first** assumption (i.i.d) is violated, since the probability of picking one is different when compared to others.
- If a model that gives buying suggestions and contains retail sales information throughout the year based on the fact that user's buying pattern changes seasonally, then the **second** assumption (stationary) is ignored, because the distribution changes over time. 

When any of the 3 assumptions is violated, we need to take careful attention to metrics (quantitative ways of measuring, comparing, tracking performance).
  



---
Tags:
[[19-03-2022]], [[Notas]], [[Curso]]