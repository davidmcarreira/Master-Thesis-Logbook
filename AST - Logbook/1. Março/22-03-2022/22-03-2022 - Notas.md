# Machine Learning - Google Crash Course
## Linear Regression with Synthetic Data - Exercise
This exercise studies the influence of hyperparameters such as (learning rate, number of epochs and batch size) in the overall performance of a model.

An **epoch** is a full training pass over the **entire data set,** such that each example (item in the data set) has been seen once. Therefore, an epoch represents $\frac{N}{batch size}$ size training iterations (one iteration means that the weights have been updated one time), where N is the total number of examples.

In the case of the exercise, we have a data set with **12 examples**, this means that:
- For a batch size of **1**, there will be $\frac{12}{1}$ iterations and the **weights will be updated 12 times**.
- On the other end, for a batch size of **12**, there will be 1 iteration and the **weights will be updated just one time**.

To support some conclusions achieved in [[20-03-2022 - Notas#^7d6b18 | 20-03-2022]], here are a few rules of thumb:
- Training loss should steadily decrease, steeply at first and then more gradually (until it approaches zero);
- If the training loss **doesn't converge**, train for more epochs;
- If the training loss is **decreasing too slowly**, the learning rate needs to be increased. *BUT* if it is set too high, the loss function might oscillate and never converge.
- If the training loss is **varies too much** (oscillator behavior), then the learning rate needs to be decreased.
- **Lowering the learning rate** while **increasing the number of epochs or batch size** is often a good combination for a more efficient training.
- A **batch size too low** can also cause instability. First, we should try with bigger values and gradually decrease it until degradation appears.
- For real-world data sets with a **lot of examples**, it might not fit into memory. Therefore, we need to reduce the batch size, so it can fit into memory.

-> The **ideal combination** of hyperparameters is data dependent, consequently, experimentation is mandatory.


<u>Useful to know</u>: 

**Transfer learning** consists of taking features learned on one problem, and leveraging them on a new, similar problem that might benefit from previous features, for example. It is usually deployed on tasks where the dataset has too little data to train a full-scale model from scratch. **Fine-tuning** consists of re-training a model obtained from transfer learning with a very low learning rate. This can achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.

Keras has 2 ways of building models: sequential or functional. The **Sequential** API allows the user to create a model layer-by-layer in a linear stack of layers, meaning that it does not allow layer sharing. Alternatively, the **Functional** API allows the creation of a model with layer sharing capabilities, meaning it is more flexible since a layer can connect to more than the previous or next layers.

A **compiler** is a computer program that transforms source code written in a programming language into another computer language. It's a translator that transforms an input source code (written towards human readability) into an output program compressed form (targeted towards execution by machines).

The **Mean Squared Error** measures the loss, but the variables are squared, therefore, we need to square root it (**Root Mean Squared Error**).

<u>Example of a dataset:</u>
```python
my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0])
my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])
```

It consists of 12 examples, where each example has one **feature** and one **label**.

&nbsp;

Before proper training the results look like this:
<p float="left">
	<img src="https://i.ibb.co/J7B8f7v/1.png" width=345>
	<img src="https://i.ibb.co/1RGvMNy/2.png" width=345>
</p>
The red line should be the linear regression of the dots, i.e, our prediction, but it is far off from the right answer and the loss function (on the right) doesn't even converge.

After tuning the hyperparameters, we have the following results:
<p float="left">
	<img src="https://i.ibb.co/FzYkQZm/3.png" width=345>
	<img src="https://i.ibb.co/LvgVgt6/4.png" width=345>
</p>

Now the linear regression is correct and the loss function actually converges.


---
Tags:
[[22-03-2022]], [[Curso]], [[Exerc√≠cios]]