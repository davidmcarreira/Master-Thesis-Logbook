# Machine Learning - Google Crash Course
## Training and Test Sets
As mentioned before, a test set is a data set that is used to evaluate the performance of a model trained by a training set.

If we only have one data set available, then we need to partition it as follows:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/PartitionTwoSets.svg"/>
</p>

Before separating, usually it is a good measure to **randomize** the whole set in the first place in order to guarantee that specific types of data don't end up in only one of the divisions (all the cats end up in the training set and the test set is composed of all the other animals in the data set).

The **test set** needs to meet the 2 following requirements:
1. It needs to be large enough to yield statistically meaningful results.
2. It is representative of the data set as a whole, i.e, as the same characteristics as the training set.

&nbsp;

Once again, we have the generalization question is raised. We need to assure that the model generalizes well to new data, therefore, the test set will act as a great proxy for that said data. Let's see an example of a model that does good in the training data as well in the test data:
<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/TrainingDataVsTestData.svg"/>
</p>


Albeit it is a very simple model, it does a fairly good job in predicting the outcomes in both sets. This fact gives us some degree of confidence to deploy it in new random data.

&nbsp;
 
A very important thing to have in mind is to **never train the model on test data**, it will completely manipulate the results, leading us to believe that we have an 100% success rate and 0 loss. And also, every time the results are looking very promising (too good to be true, even), they shall be approached with care as they might indicate that test data has leaked into the training set.
 
After training, the model achieves 99% precision on both the training set and the test set. We'd expect a lower precision on the test set, so we take another look at the data and discover that many of the examples in the test set are duplicates of examples in the training set (we neglected to scrub duplicate entries for the same spam email from our input database before splitting the data). We've inadvertently trained on some of our test data, and as a result, we're no longer accurately measuring how well our model generalizes to new data.
 
&nbsp;
&nbsp;

<u>**Example**</u>:
 For example, let's consider a model that predicts if an email is considered spam, through features as the subject line, email body and sender's email. We partition the data set in an 80-20 split for training and test data, respectively.  After training, we see a 99% precision on both the training and test set, which look too optimistic. After taking a look at the data, we discover that there are duplicates in the test set that were also in the training data. Basically, we committed the biggest sin of them all: trained in the test set. Inadvertently, yes, but now the model can't be trusted and needs to be trained **all over again**.
 
 <u>**Tasks**</u>
 
**Task 1**
For a learning rate of 3, batch size of 1 and a 50% percentage of training data, then the delta loss is -0.040.

**Task 2**
-0.040 -> learning rate: 3; 50%; batch size: 1
-0.063 -> learning rate: 1; 50%; batch size: 1
-0.001 -> learning rate: 0.3; 50%; batch size: 1
-0.040 -> learning rate: 0.1; 50%; batch size: 1 
-0.030 -> learning rate: 0.03; 50%; batch size: 1 
-0.033 -> learning rate: 0.01; 50%; batch size: 1 
-0.036 -> learning rate: 0.003; 50%; batch size: 1
-0.035 -> learning rate: 0.001; 50%; batch size: 1

If we increase the batch size:
-0.031 -> learning rate: 3; 50%; batch size: 30  
-0.063 -> learning rate: 1; 50%; batch size: 30  
-0.039 -> learning rate: 0.3; 50%; batch size: 30  
-0.037 -> learning rate: 0.1; 50%; batch size: 30  
-0.036 -> learning rate: 0.03; 50%; batch size: 30 
-0.036 -> learning rate: 0.01; 50%; batch size: 30
-0.036 -> learning rate: 0.003; 50%; batch size: 30
-0.036 -> learning rate: 0.001; 50%; batch size: 1

Increasing batch size and learning rate yield similar delta loss results.
For a batch size of 1 and learning rate of 0.1, the delta loss is -0.054. If we increase the batch size to 20 while maintaining the learning rate the delta loss is -0.068. To decrease the delta, we must increase the learning rate.

<u>**Conclusion**</u>: Bigger batch size means bigger learning rate, because the margin for error is also bigger (we can take bigger steps). ^7d6b18

**Task 3**
Altering the percentage split between training and test data, alters completely the optimal learning settings. Decreasing so much the percentage of training data means a badly trained model that won't be able to work properly, yielding very low losses related to training but high test losses (at least compared to the training ones).

&nbsp;  
&nbsp;  
&nbsp;  

 
---  
Tags:
[[20-03-2022]], [[Curso]]