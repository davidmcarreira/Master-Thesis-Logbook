# Machine learning Crash Course - Google #
## Descending into ML
### Linear Regression
When a linear regression is performed on a real set of data, the line will not pass straight through all the dots. Let's take as an example the air temperature in Celsius vs the cricket chirps per minute. 

<p align="center">
	<img src="https://developers.google.com/machine-learning/crash-course/images/CricketLine.svg" class="center" width=500 height="auto"/>
</p>

The line equation is, obviously:
$$
y=mx+b
$$

where:
- $y$ is the temperature in Celsius -> the value we're trying to predict;
- $m$ is the slope of the line;
- $x$ are the chirps per minute;
- $b$ is the y-intercept.

By convention, in a machine learning context, the previous equation will look mote like this:
$$
y'=b+w_1x_1
$$

where:
- $y'$ is the predicted **label** (desired output);
- $b$ is the **bias** (sometimes referred to as $w_0$ is an intercept or offset from an origin) $\neq$ prediction bias (how far the average of predictions is from the average of labels in the dataset);
- $w_1$ is the **weight of the feature 1** (weight is the same concept as "slope" $m$);
- $x_1$ is a **feature** (a known input).

More sophisticated models can use multiple features, each with their own weight. For example:
$$
y'=b+w_1x_1+w_2x_2+w_3x_3+...+w_nx_n
$$

### Training and Loss
Training a model is just finding good values for the weights and bias by using labeled examples. In Supervised Learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss. This process is called **empirical risk minimization**.

Like the name says, loss is a penalty for a **bad prediction**. It's a quantitative object that indicates how bad the model's prediction was for a single example. A perfect prediction has zero loss, otherwise, the value is a positive quantity. The main goal of **training** a model is to find a set of weights and biases that **minimize the loss** across **all** the examples.

<img src="https://developers.google.com/machine-learning/crash-course/images/LossSideBySide.png">

- The dots are the real values;
- The arrows represent loss;
- the blue line are predictions.

In the left model it is clear the loss is high, while in the right model we have low loss.

One of the ways to quantify loss is through a **loss function**. For linear regression models, a popular example is the **Mean Square Error (MSE)**, that will compute the square of the difference between the label and the prediction for every iteration:
$$
MSE=\frac{1}{N}\sum_{(x, y) \in D}(y - y'(x))^2
$$
where:
- $(x,y)$ is an example which:
	- $x$ are the features used to make predictions;
	- $y$ are the labeled examples.
- $y'(x)$ is a function of the weights and bias combined with the features $x$;
- $N$ is the number of examples in $D$.

MSE is a commonly-used loss function in Machine Learning, even though it isn't the more practical nor the best for all circumstances.

