# Machine Learning Crash Course - Google #

## Introduction
Machine Learning approaches allow programmers to do 3 things better:
- Reduce time spent programming (an autocorrector can be made from scratch in a few weeks, but through artificial intelligence we can feed some examples to a model and have an even more reliable program in a small fraction of time).
- Allows product customization, making better for more people (if we took the previous example and wanted to expand it to the 100 more popular languages, we would need to start almost from scratch for each one. Using machine learning, we would just need to give it examples in those languages and let the model handle it)
- Machine learning allows programs to tackle problems that we wouldn't need how to solve them naturally (we can recognize friends subconsciously by distinguish faces and specific personal traits, but how could a handmade program do that? Where would we start? What would we look for?)

Programming is a matter of logic, while Machine Learning can be seen as statistics.

## Framing
**Label** is the variable we're predicting (represented by the variable **y**) It's the target we're trying to predict.

**Features** are input variables describing our data (represented by the variables {$x_1$, $x_2$, $x_3$, $...$, $x_n$}). Pieces of information extracted from the input data that can represent/identify it.

Example:
- Label: Spam/Not Spam
- Feature: To and from addresses, header information, keywords ("click", "link").

***Example*** is an instance of data, **x**. We can have:
- Labeled examples (used to train models) that have `{features, label}: (x, y)`.
- Unlabeled examples (used to predict new data) that are `{features, ?}: (x, ?)`.

**Model** is what will do the predicting and has the function of mapping examples in order to predict labels, **y'**. It defines the relation between features and labels. A model can be divided in 2 phases:
- **Training** - creating or learning the model by showing it labeled examples. Gradually, its internal parameters will adapt to learn the relationship between features and label.
- **Inference** - applying of the trained model to unlabeled examples. It's the stage of predicting the outcome by attributing it a predicted label **y'**.

### Regression vs Classification
A regression model predicts **continuous values,** while a classification model predicts **discrete values**.

For example:
- The value of a house based on features like the number of bedrooms and square meters -> Regression.
- Is this a dog or a cat? → Classification.
- What is the probability of a user clicking on an ad? → Regression.
- The *citrus* dilemma -> Classification.

<br />


## Descending into ML

![](LossSideBySide.png)

---
Tags:
[[14-03-2022]], [[Notas]], [[Curso]]